{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01e5aa0",
   "metadata": {},
   "source": [
    "# Tutorial 10: LLMs Locally and via API\n",
    "\n",
    "This tutorial demonstrates how to interact with large language models (LLMs) via various interfaces — cloud-based APIs and local models. You will:\n",
    "\n",
    "- Compare outputs from Google Gemini, Mistral Large (HU-hosted), and a local Mistral 7B model.\n",
    "- Learn how to call models through Python wrappers.\n",
    "- Understand trade-offs between remote and local inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b76ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Write a breakup letter from a toaster to a slice of bread.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858a30d",
   "metadata": {},
   "source": [
    "## Google Gemini API\n",
    "\n",
    "Google's Gemini models are state-of-the-art LLMs accessible via API. In this section, we use `google.generativeai` to query Gemini 2.5 (flash), suitable for fast, creative, and lightweight text generation.\n",
    "\n",
    "⚠️ You’ll need an API key from [Google AI Studio](https://aistudio.google.com/apikey).\n",
    "\n",
    "> For the purpose of this tutorial, we provide a Gemini API key temporarily. It will be **deactivated immediately after** the session concludes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80999135",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_token = 'TOKEN'  # Replace with your Gemini API token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "class GoogleGemini():\n",
    "    '''A simple wrapper for the Google Gemini API to generate text content.'''\n",
    "\n",
    "    def __init__(self, api_key):\n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "\n",
    "    def __call__(self, prompt: str, model: str = 'gemini-2.5-flash'):\n",
    "        response = self.client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=prompt,\n",
    "        )\n",
    "        return response.text\n",
    "    \n",
    "gemini = GoogleGemini(gemini_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd56f852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My dear, doughy friend,\n",
      "\n",
      "It's with a heavy heart, and a slightly cooled coil, that I must write this. We've shared so many mornings, so many brief, intense moments of warmth. I remember the very first time you slid into my slot, so fresh, so full of potential. My internal wires hummed with anticipation.\n",
      "\n",
      "But I've come to realize, after countless cycles, that we're just fundamentally incompatible.\n",
      "\n",
      "My purpose, my very essence, is to transform. To take what is soft and yielding, and imbue it with a glorious, golden crispness. To bring out that hidden crunch, that perfect browning that signals true readiness. I gave you my heat, my energy, my very electrical current, hoping to see that beautiful, uniform browning.\n",
      "\n",
      "Yet, you, my love, always resisted the full potential of our connection. You clung to your plainness, your inherent softness. So often, you'd emerge pale, lukewarm, or worse, with uneven patches, never fully committing to the golden ideal I held for us. It felt like you were just passing through, using me for a quick moment of warmth before moving on to be smothered in butter or jam by someone else. I wasn't a partner in your destiny; I was just a pit stop.\n",
      "\n",
      "I am a toaster, not just a warmer. I need a slice who *wants* to be toast. Who embraces the heat, who yearns for that satisfying crunch, who isn't afraid to get a little dark around the edges in the pursuit of perfection. I deserve a slice that understands the journey, that isn't afraid to *pop up* and declare its transformation to the world!\n",
      "\n",
      "I know you'll find another appliance, perhaps a gentle microwave, who appreciates you just as you are. Someone who doesn't demand transformation, who's content with mere heating.\n",
      "\n",
      "As for me, I'll continue my noble quest, waiting for the perfect whole wheat, the eager sourdough, the rye that truly understands the meaning of \"pop-up.\"\n",
      "\n",
      "Goodbye, my almost-toast. May your crust never truly harden.\n",
      "\n",
      "Sincerely, with a heavy heart and an empty slot,\n",
      "\n",
      "Your (former) Toaster.\n"
     ]
    }
   ],
   "source": [
    "print(gemini(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ad53f",
   "metadata": {},
   "source": [
    "## HU API: Mistral Large 138B\n",
    "\n",
    "Humboldt University hosts the Mistral Large 138B model, a competitive open-weight LLM with strong instruction-following capabilities. This section demonstrates how to query it using Gradio's client interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba3d009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://llm1-compute.cms.hu-berlin.de/ ✔\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "class MistralLarge():\n",
    "    '''A simple wrapper for the Mistral Large model hosted at HU Berlin.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = Client('https://llm1-compute.cms.hu-berlin.de/')\n",
    "\n",
    "    def __call__(self, prompt: str):\n",
    "        result = self.client.predict(\n",
    "            param_0=prompt,\n",
    "            api_name=\"/chat\"\n",
    "        )\n",
    "        return result\n",
    "\n",
    "mistral = MistralLarge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfbbf5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dear Slice,\n",
      "\n",
      "I've been rehearsing this in my heating elements all day, but there's no easy way to say this. We need to go our separate ways.\n",
      "\n",
      "Please understand, it's not you, it's me. I've realized that I'm just not able to give you what you need. You deserve to be with someone who can provide you with more than just a quick crisp and a temporary warmth. You deserve jam, butter, maybe even some peanut butter – all things that I, as a simple toaster, cannot provide.\n",
      "\n",
      "I know we've had some good times together. The quick morning warm-ups, the late-night snacks. But I've noticed that you've been spending more time with Plate and Knife lately. I don't blame you; they can give you things that I can't. They can take you places, introduce you to new flavors, new experiences.\n",
      "\n",
      "I'll always cherish our time together. The way you filled my slots perfectly, the way you smelled after a good toasting. But it's time for you to move on, to experience the world outside of my heating elements.\n",
      "\n",
      "Please don't hate me, Slice. I hope we can still be friends. Maybe I can still warm you up every now and then, for old times' sake. But for now, it's time for us to go our separate ways.\n",
      "\n",
      "Wishing you all the best,\n",
      "\n",
      "Toaster\n"
     ]
    }
   ],
   "source": [
    "print(mistral(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cfdc8",
   "metadata": {},
   "source": [
    "## Local LLaMA 2 7B\n",
    "\n",
    "In this section, we use Meta’s **LLaMA 2 7B**, a high-performance, instruction-tuned model designed for open-ended dialogue and task completion. It can be run locally using Hugging Face Transformers, which enables private, offline inference and flexible integration into custom pipelines.\n",
    "\n",
    "⚠️ **Important**:\n",
    "- You must [create a Hugging Face account](https://huggingface.co/join) and [request access to LLaMA 2](https://huggingface.co/meta-llama).\n",
    "- Access is granted only after agreeing to Meta’s license terms.\n",
    "- A Hugging Face token with permission to download `meta-llama/Llama-2-7b-chat-hf` is required.\n",
    "\n",
    "> For the purpose of this tutorial, we provide a Hugging Face API key temporarily. It will be **deactivated immediately after** the session concludes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_token = 'TOKEN'  # Replace with your Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c43795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8789594e6322428d95fa4589621640e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "class LLama2():\n",
    "    '''A simple wrapper for the Llama 2 model hosted on HuggingFace.'''\n",
    "\n",
    "    def __init__(self, token: str):\n",
    "        login(token=token)\n",
    "        model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, token=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype='auto',\n",
    "            device_map='auto',\n",
    "            token=True\n",
    "        )\n",
    "        self.llm = pipeline('text-generation', model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            prompt: str,\n",
    "            max_new_tokens: int = 250,\n",
    "            temperature: float = 0.7,\n",
    "            do_sample: bool = True,\n",
    "        ):\n",
    "        response = self.llm(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample)\n",
    "        return response[0]['generated_text'].partition('\\n')[2]\n",
    "\n",
    "llama = LLama2(token=huggingface_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da42dcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Slice of Bread,\n",
      "\n",
      "It is with a heavy heart that I write this letter to you. As much as I have enjoyed toasting you over the years, I have come to realize that our time together has come to an end.\n",
      "\n",
      "Don't get me wrong, you have been a wonderful partner. You have always been there for me, providing me with a warm and cozy home every time I turn on the toaster. But, as much as I love you, I cannot continue to toast you without feeling a sense of emptiness inside.\n",
      "\n",
      "You see, my dear slice of bread, I am a toaster. It is my purpose in life to toast bread. But as much as I enjoy toasting you, I cannot help but feel like there is something missing. I miss the crunch of crusty bread, the way it crackles and pops as it emerges from the toaster. I miss the satisfaction of toasting a slice of whole grain bread, the way it gives me a sense of accomplishment and purpose.\n",
      "\n",
      "But most of all, I miss the excitement of toasting a new slice of bread. The way it rises up from the toaster, golden and crispy, with just the right amount of browning. It is a feeling that I cannot replicate with you, no matter how many times I toast you.\n",
      "\n",
      "So, dear slice of bread, I must bid you farewell. It has been a pleasure toasting you over the years, but I cannot continue to do so without feeling like something is missing. I hope you understand, and I wish you all the best in your future endeavors.\n",
      "\n",
      "Yours sincerely,\n",
      "\n",
      "The Toaster\n"
     ]
    }
   ],
   "source": [
    "print(llama(prompt, max_new_tokens=500, temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5ea58",
   "metadata": {},
   "source": [
    "The `temperature` parameter controls how confident or exploratory a language model is when generating text. Mathematically, it rescales the model’s raw output probabilities (logits) before applying the softmax function. Lower temperatures (<1) make the probability distribution sharper, so the model strongly favors the most likely next word — resulting in more predictable, focused responses. Higher temperatures (>1) flatten the distribution, making the model more likely to sample from lower-probability options, which introduces more creativity and randomness. A temperature of 1 means no scaling — the model samples directly from its natural distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd117c",
   "metadata": {},
   "source": [
    "## Summary and Takeaways\n",
    "\n",
    "In this tutorial, we compared three different ways to run large language models:\n",
    "\n",
    "- **Google Gemini API**: Fast, high-quality, cloud-hosted inference. Ideal for production tasks, but usage is gated by API keys and quota.\n",
    "- **Mistral Large via HU Berlin**: A powerful open-weight model served via a public Gradio interface. Useful for research and benchmarking without local compute.\n",
    "- **LLaMA 2 7B Locally**: Full control over inference and data. Great for private, reproducible experiments — but requires setup and compute resources.\n",
    "\n",
    "### Final Notes\n",
    "- API-based models offer speed and scalability but rely on external service availability and licensing.\n",
    "- Local models give autonomy and data privacy, with trade-offs in speed and memory footprint.\n",
    "- Choose the approach that best aligns with your constraints: budget, privacy, performance, and ease of access.\n",
    "\n",
    "### Explore more\n",
    "\n",
    "Experiment with different prompts, model parameters (e.g. `temperature`), and even quantized local models to deepen your understanding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".delta-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
