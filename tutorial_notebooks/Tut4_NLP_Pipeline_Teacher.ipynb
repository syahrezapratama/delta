{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4 - Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "You are already familiar with building predictive models on tabular data. With the latter, you have a feature matrix `X` and a target vector `Y`. Given these data structures, you can apply ML algorithms to learn the relationship between `X` and `Y`. In this notebook, you will work with a dataset consisting of movie reviews, each labeled with either a negative or positive sentiment. In contrast to numerical tabular data, textual data cannot be fed directly into ML algorithms for predictive modeling purposes. Thus, with textual data, you need to preprocess each data sample to obtain the required feature matrix `X`. This processing of the data is what we call the \"NLP pipeline\". The dimensionality and the quality of the features, which can be extracted from textual data, depend on the preprocessing steps implemented in the NLP pipeline. In this notebook, we will mainly focus on NLP preprocessing and frequency-based feature creation.<br>\n",
    "\n",
    "Here is the outline of today's notebook:\n",
    "*   NLP Processing Pipeline (Demo). \n",
    "*   Preprocessing of Movie Reviews Data (Exercise 1).\n",
    "*   Frequency-based Feature Creation (Bag-of-Words) for Text Classification (Exercise 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import recall_score,precision_score,roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. NLP Processing Pipeline** (DEMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will cover key data preprocessing steps of an NLP pipeline. First, let's revisit the latter with the visualization below:<br>\n",
    "\n",
    "<img src=\"https://github.com/Humboldt-WI/demopy/raw/main/NLP_Pipeline_Overview.PNG\" width=\"950\" height=\"520\" alt=\"NLP Pipeline\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by gathering samples for our textual data corpus, i.e., we go through the archives of, e.g., a company, and pull out Word files, PowerPoint presentations, etc. Since the machine learning models we have learned so far such as neural networks, decision trees, random forest, etc., do not natively process text, we need to go through specific data preprocessing and cleaning steps to convert textual data into a format that our algorithms can work with. Let's assume we have the following movie review:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<br>Last week, I went to the Berlin movie theater Zoo Palast, https://zoopalast.premiumkino.de, to watch the new action movie with Tom Cruise, which turned out to be very enjoyable experience. While there was a long queue at the entrance of the movie theater and I had to wait around 10 - 15 min. to go in, the movie was really worth it!<br>\n"
     ]
    }
   ],
   "source": [
    "movie_review='<br>Last week, I went to the Berlin movie theater Zoo Palast, https://zoopalast.premiumkino.de, to watch the new action movie with Tom Cruise, which turned out to be very enjoyable experience. While there was a long queue at the entrance of the movie theater and I had to wait around 10 - 15 min. to go in, the movie was really worth it!<br>'\n",
    "print(movie_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our goal is to determine the sentiment of the movie review, then the HTML content, i.e., `< br >` indicating a line break, and the URL link would not provide us with any useful information. Therefore, the first two data preprocessing steps, which we will take, are to remove the HTML content and to filter out the URL from our textual sample:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Step: Removing URL Link\n",
      "<br>Last week, I went to the Berlin movie theater Zoo Palast,  to watch the new action movie with Tom Cruise, which turned out to be very enjoyable experience. While there was a long queue at the entrance of the movie theater and I had to wait around 10 - 15 min. to go in, the movie was really worth it!<br> \n",
      "\n",
      "2. Step: Removing HMTL Content\n",
      "Last week, I went to the Berlin movie theater Zoo Palast,  to watch the new action movie with Tom Cruise, which turned out to be very enjoyable experience. While there was a long queue at the entrance of the movie theater and I had to wait around 10 - 15 min. to go in, the movie was really worth it!\n"
     ]
    }
   ],
   "source": [
    "# Remove html content\n",
    "print('1. Step: Removing URL Link')\n",
    "clean_movie_review = re.sub(\"http\\S+\", \"\",movie_review)\n",
    "print(clean_movie_review,'\\n')\n",
    "\n",
    "print('2. Step: Removing HMTL Content')\n",
    "clean_movie_review = BeautifulSoup(clean_movie_review).get_text()\n",
    "print(clean_movie_review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we make use of *regular expressions* to clean the text. Regular expressions are a powerful mechanism for text processing. However, a comprehensive discussion of what you can do with regular expressions and how to use them is out of the scope of this notebook. If you'd like to learn more about regular expression, check out the [Wikipedia page](https://en.wikipedia.org/wiki/Regular_expression) as a starting point. The [W3Schools](https://www.w3schools.com/python/python_regex.asp) website or the [Regex101](https://regex101.com/) website provide easily accessible playgrounds to start working with regular expressions.\n",
    "\n",
    " In our demo, we use the python package for regular expressions, i.e., `re`, to remove unnecessary characters. To achieve this, we specify a pattern that we would like to remove, i.e, \"http\\S+\", and we also specify the replacement, i.e, an empty character. \"\\S+\" matches a word or token that does not contain any whitespace. In combination with \"http\", we essentially look for a substring in our textual sample that starts with \"http\" and is followed by one or more non-whitespace characters. Put in other words, the pattern will find everything from \"http\" until the next space, and remove the corresponding content. Additionally, the package `BeautifulSoup` facilitates the scraping of information from web pages. When we feed text with any HMTL content to `BeautifulSoup` we use the function `get_text()` to retrieve the HTML tag-free version of our textual sample. Besides HTML and URL content, we would also like to remove any non-alphabetic characters, as they usually do not carry any semantic information:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Step: Removing non-alphabetic Characters\n",
      "Last week  I went to the Berlin movie theater Zoo Palast   to watch the new action movie with Tom Cruise  which turned out to be very enjoyable experience  While there was a long queue at the entrance of the movie theater and I had to wait around         min  to go in  the movie was really worth it \n"
     ]
    }
   ],
   "source": [
    "print('3. Step: Removing non-alphabetic Characters')\n",
    "clean_movie_review = re.sub(\"[^a-zA-Z]\", \" \",clean_movie_review)\n",
    "print(clean_movie_review)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bracket list \"[^...]\" will find any set of characters that are not matched (\"^\") by the specified pattern (\"...\"). Thus, by setting \"[^a-zA-Z]\", we are looking for all substrings, that do not overlap with lower case or upper case alphabet characters. Since letter casing (whether upper or lower) does not provide any information about the underlying meaning or context of the textual data, we will transform the whole movie review to lower case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Step: Transforming all Characters to lower Case\n",
      "last week  i went to the berlin movie theater zoo palast   to watch the new action movie with tom cruise  which turned out to be very enjoyable experience  while there was a long queue at the entrance of the movie theater and i had to wait around         min  to go in  the movie was really worth it \n"
     ]
    }
   ],
   "source": [
    "print('4. Step: Transforming all Characters to lower Case')\n",
    "clean_movie_review = clean_movie_review.lower()\n",
    "print(clean_movie_review)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, before we proceed to splitting our movie review into a list of words using the package `nltk`, we will revisit the concept of tokenization with the visualization below:<br>\n",
    "<img src=\"https://github.com/Humboldt-WI/demopy/raw/main/Tokenization.PNG\" width=\"950\" height=\"440\" alt=\"Tokenization\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Step: Performing Tokenization\n",
      "['last', 'week', 'i', 'went', 'to', 'the', 'berlin', 'movie', 'theater', 'zoo']\n"
     ]
    }
   ],
   "source": [
    "print('5. Step: Performing Tokenization')\n",
    "clean_movie_review=nltk.word_tokenize(clean_movie_review)\n",
    "print(clean_movie_review[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting list of tokens directly impacts the feature extraction process. Therefore, before splitting the movie review into individual words, we removed some unnecessary content. However, when we consider all tokens put together, their number can still very easily turn large. To further reduce this number, we eliminate the so-called stopwords. The latter usually do not contribute to the modelâ€˜s perception of the movie review, whether it has a negative or a positive sentiment:<br>\n",
    "<img src=\"https://github.com/Humboldt-WI/demopy/raw/main/Removal_Stopwords.PNG\" width=\"950\" height=\"530\" alt=\"Stopwords\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Georg\n",
      "[nltk_data]     Velev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') ## to download stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for Stopwords from NLTK:  ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n"
     ]
    }
   ],
   "source": [
    "print('Examples for Stopwords from NLTK: ',stopwords.words(\"english\")[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stopwords from `nltk` represent a list of tokens, which we will exclude from our textual sample. If we have a reason to believe that some of the tokens in the stopwords list could hold a semantic meaning for our task, we would first remove these tokens before the stopwords elimination. Adjustments could also be performed in the opposite case, when we decide to extend the list of stopwords with some specific tokens, which are not covered by `nltk`, and are yet contained in our textual data. After all, and as shown in the previous, `nltk` maintains the list of stopword list as a Python `list`. You can manipulate this list just as any other `list` using its inbuilt methods like `append()`, `remove()`, etc. For example, if you want to add the token \"yet\" to the list of stopwords, you can do so by calling `stopwords.append(\"yet\")`. In our demo, we will use the default stopword list provided by `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Step: Stopwords Removal\n",
      "- Number of Tokens before Stopwords Removal:  57\n",
      "- Number of Tokens after Stopwords Removal:  29\n"
     ]
    }
   ],
   "source": [
    "all_tokens=len(clean_movie_review)\n",
    "print('6. Step: Stopwords Removal')\n",
    "clean_movie_review = [w for w in clean_movie_review if w not in stopwords.words(\"english\")]\n",
    "print('- Number of Tokens before Stopwords Removal: ', all_tokens)\n",
    "print('- Number of Tokens after Stopwords Removal: ', len(clean_movie_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the elimination of stopwords, we can further reduce number of distinct tokens with stemming and lemmatization:<br>\n",
    "<img src=\"https://github.com/Humboldt-WI/demopy/raw/main/Stemming_Lemmatization.PNG\" width=\"950\" height=\"530\" alt=\"Stemming_Lemmatization\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to \"understand\" text algorithmically, it might be enough to know that *someone performs the action of going somewhere*, whereas it might matter less whether they are going now, went in the past, or will go in the future. While stemming is a crude heuristic to remove the end or the beginning of a word based on pre-defined suffixes and prefixes, respectively, lemmatization normalizes tokens by reducing words to their dictionary form. Since stemming can produce non-existent words, lemmatization is the better choice for ensuring consistency in how we represent text. Determining the grammatical role, i.e., a token's part-of-speech tag, can improve the results obtained with lemmatization. Compared to stemming, however, lemmatization is a more complex step and requires, for example, a POS tagger:<br>\n",
    "<img src=\"https://github.com/Humboldt-WI/demopy/raw/main/POS.PNG\" width=\"950\" height=\"530\" alt=\"POS\">\n",
    "<br>\n",
    "The following functions illustrates POS tagging with the help of the `nltk` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Georg\n",
      "[nltk_data]     Velev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize with POS Tag (Parts of Speech tagging)\n",
    "nltk.download('wordnet')\n",
    "def get_wordnet_pos(word:str)->str:\n",
    "    \"\"\"Map POS tag to first character for lemmatization\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pos: str\n",
    "        The positional tag of speech retrieved from wordnet database.\n",
    "    \"\"\"\n",
    "\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "                \n",
    "    pos=tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. Step: Lemmatization with POS Tagging:\n",
      "- First three Tokens before Lemmatization:  ['last', 'week', 'went']\n",
      "- First three Tokens after Lemmatization:  ['last', 'week', 'go']\n"
     ]
    }
   ],
   "source": [
    "print('7. Step: Lemmatization with POS Tagging:')\n",
    "print('- First three Tokens before Lemmatization: ',clean_movie_review[:3])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_movie_review=[lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in clean_movie_review]\n",
    "print('- First three Tokens after Lemmatization: ',clean_movie_review[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `lemmatize` from the class `WordNetLemmatizer` retrieves the dictionary form of the input token from the WordNet database. The latter represents a large english lexical database that has been continuously extended over the years. WordNet groups words together based on their semantic meaning forming synonym sets, also known as synsets. Most of the relations in WordNet connect words from the same POS. For more details on WordNet, we refer the reader to https://wordnet.princeton.edu/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo Summary**:<br>\n",
    "- the first five steps of our NLP preprocessing pipeline involve the elimination of HTML content, URLs, non-alphabetic characters, converting all tokens to lower case, and tokenization.\n",
    "- once the text is split into individual tokens, we apply two techniques to further reduce the number of words: stopwords removal and lemmatization. The latter reduces the number of distinctive words per data sample by replacing different forms of the same token with its dictionary form. This is essential for frequency-based feature extraction techniques, as each textual sample is represented with a numerical vector, the dimensionality of which is determined by the number of distinct tokens in the entire vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Preprocessing of Movie Reviews Data** (1. Exercise)<br>\n",
    "- put the individual preprocessing steps from the demo in a well documented function. The latter should take as inputs a single textual sample.\n",
    "- preprocess the first 1,000 movie reviews from the IMDB dataset. For this purpose, make use of the `apply()` function in `pandas` to transform each movie review using the NLP preprocessing pipeline function.\n",
    "- split your data into 80% train and 20% test subsets using the function `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_preprocessing_pipeline(textual_sample:str)->list:\n",
    "    '''\n",
    "    Implements 7 steps of an NLP preprocessing pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    textual_sample:str\n",
    "        The input text that requires preprocessing\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    preprocessed_textual_sample:list\n",
    "        The textual sample after each of the 7 preprocessing steps have been applied.\n",
    "\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    #Removing of URLs:\n",
    "    preprocessed_textual_sample = re.sub(\"http\\S+\", \"\",textual_sample)\n",
    "\n",
    "    #Removing of HMTL tags:\n",
    "    preprocessed_textual_sample = BeautifulSoup(preprocessed_textual_sample).get_text()\n",
    "\n",
    "    #Removing of non-alphabetic characters:\n",
    "    preprocessed_textual_sample = re.sub(\"[^a-zA-Z]\", \" \",preprocessed_textual_sample)\n",
    "\n",
    "    #Changing all tokens to lower case:\n",
    "    preprocessed_textual_sample = preprocessed_textual_sample.lower()\n",
    "\n",
    "    #Tokenization:\n",
    "    preprocessed_textual_sample=nltk.word_tokenize(preprocessed_textual_sample)\n",
    "\n",
    "    #Stopwords removal:\n",
    "    preprocessed_textual_sample = [w for w in preprocessed_textual_sample if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    #Lemmatization:\n",
    "    preprocessed_textual_sample=[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in preprocessed_textual_sample]\n",
    "    return preprocessed_textual_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"IMDB-50K-Movie-Review.zip\", sep=\",\", encoding=\"ISO-8859-1\").iloc[:1000,:]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Calling our preprocessing functions to clean the reviews (caution, this step can take some time)\n",
    "X=df['review'].apply(NLP_preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['sentiment'].map({'positive':1,'negative':0}).values  # map text-based class labels to numbers\n",
    "Xclean_train, Xclean_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5)  # data partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Frequency-based Feature Creation (Bag-of-Words) for Text Classification** <br>(2. Exercise)<br>\n",
    "- extract frequency-based features from your training and test sets using two alternative techniques: `TfidfVectorizer` (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and `CountVectorizer` (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer). Both classes are available in the `sklearn` library. Similar to data preprocessing in tabular datasets, you will call the function `fit_transform()` to extract frequency features on the training set, whereas you will apply the function `transform()` to your test set. Also, since you have already applied your custom preprocessing function to the moview reviews dataset, you can pass a dummy function as an argument to the input parameters  `tokenizer` and `preprocessor` of `TfidfVectorizer` and `CountVectorizer`. In this way, the vectorizers would not clean the data, but would only extract frequency-based features.\n",
    "- train and test the following algorithms on the two resulting feature spaces: Logistic Regression and XGBClassifier\n",
    "- evaluate the predictions on the test set in terms of the AUC, recall, and precision, and store all results in a single pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc  \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = dummy_fun,\n",
    "    preprocessor = dummy_fun,\n",
    "    token_pattern = None)\n",
    "\n",
    "#TFIDF Feature Extraction:\n",
    "reviews_clean_tfidf_tr = tfidf_vectorizer.fit_transform(Xclean_train)\n",
    "reviews_clean_tfidf_ts = tfidf_vectorizer.transform(Xclean_test)\n",
    "\n",
    "#CountVectorizer Feature Extraction:\n",
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = dummy_fun,\n",
    "    preprocessor = dummy_fun,\n",
    "    token_pattern = None)\n",
    "\n",
    "reviews_clean_count_tr = count_vectorizer.fit_transform(Xclean_train)\n",
    "reviews_clean_count_ts = count_vectorizer.transform(Xclean_test)\n",
    "\n",
    "\n",
    "#An alternative way of preprocessing the data with TfidfVectorizer and CountVectorizer\n",
    "#would be to use the custom preprocessing function as an input parameter to the two vectorizers.\n",
    "#Since the function performs tokenization, you would pass NLP_preprocessing_pipeline as an input only to \n",
    "#the preprocessor parameter, and you will keep tokenizer = dummy_fun. This would essentially do the same \n",
    "#as you have done before: the custom NLP pipeline function would be applied to each row of the movie reviews dataset.\n",
    "#First, you would split the data into train and test subsets, and then you would apply the vectorizers, e.g.:\n",
    "\n",
    "#X=df['review']\n",
    "#Xclean_train, Xclean_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5)  # data partitioning\n",
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer(\n",
    "#    analyzer = 'word',\n",
    "#    tokenizer = dummy_fun,\n",
    "#    preprocessor = NLP_preprocessing_pipeline,\n",
    "#    token_pattern = None)\n",
    "\n",
    "#TFIDF Feature Extraction:\n",
    "#reviews_clean_tfidf_tr = tfidf_vectorizer.fit_transform(Xclean_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_results_frame=[]\n",
    "\n",
    "xgbc=XGBClassifier(n_estimators=10)\n",
    "xgbc.fit(reviews_clean_tfidf_tr,y_train)\n",
    "xgbc_tfidf_discrete_pr=xgbc.predict(reviews_clean_tfidf_ts)\n",
    "xgbc_tfidf_proba_pr=xgbc.predict_proba(reviews_clean_tfidf_ts)[:,1]\n",
    "\n",
    "xgbc.fit(reviews_clean_count_tr,y_train)\n",
    "xgbc_count_discrete_pr=xgbc.predict(reviews_clean_count_ts)\n",
    "xgbc_count_proba_pr=xgbc.predict_proba(reviews_clean_count_ts)[:,1]\n",
    "\n",
    "predictions_results_frame.append([recall_score(y_true=y_test,y_pred=xgbc_tfidf_discrete_pr),\n",
    "                                    precision_score(y_true=y_test,y_pred=xgbc_tfidf_discrete_pr),\n",
    "                                    roc_auc_score(y_true=y_test,y_score=xgbc_tfidf_proba_pr)])\n",
    "\n",
    "predictions_results_frame.append([recall_score(y_true=y_test,y_pred=xgbc_count_discrete_pr),\n",
    "                                precision_score(y_true=y_test,y_pred=xgbc_count_discrete_pr),\n",
    "                                roc_auc_score(y_true=y_test,y_score=xgbc_count_proba_pr)])\n",
    "\n",
    "logit=LogisticRegression()\n",
    "logit.fit(reviews_clean_tfidf_tr,y_train)\n",
    "logit_tfidf_discrete_pr=logit.predict(reviews_clean_tfidf_ts)\n",
    "logit_tfidf_proba_pr=logit.predict_proba(reviews_clean_tfidf_ts)[:,1]\n",
    "\n",
    "logit.fit(reviews_clean_count_tr,y_train)\n",
    "logit_count_discrete_pr=logit.predict(reviews_clean_count_ts)\n",
    "logit_count_proba_pr=logit.predict_proba(reviews_clean_count_ts)[:,1]\n",
    "\n",
    "predictions_results_frame.append([recall_score(y_true=y_test,y_pred=logit_tfidf_discrete_pr),\n",
    "                                    precision_score(y_true=y_test,y_pred=logit_tfidf_discrete_pr),\n",
    "                                    roc_auc_score(y_true=y_test,y_score=logit_tfidf_proba_pr)])\n",
    "\n",
    "predictions_results_frame.append([recall_score(y_true=y_test,y_pred=logit_count_discrete_pr),\n",
    "                                precision_score(y_true=y_test,y_pred=logit_count_discrete_pr),\n",
    "                                roc_auc_score(y_true=y_test,y_score=logit_count_proba_pr)])\n",
    "\n",
    "    \n",
    "results_overview=pd.DataFrame(np.around(np.array(predictions_results_frame),3),columns=['Recall Score','Precision Score','ROC Auc Score'],\n",
    "            index=['XGB_TFIDF','XGB_Count','Logit_TFIDF','Logit_Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recall Score</th>\n",
       "      <th>Precision Score</th>\n",
       "      <th>ROC Auc Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGB_TFIDF</th>\n",
       "      <td>0.783</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_Count</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logit_TFIDF</th>\n",
       "      <td>0.858</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logit_Count</th>\n",
       "      <td>0.821</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Recall Score  Precision Score  ROC Auc Score\n",
       "XGB_TFIDF           0.783            0.790          0.860\n",
       "XGB_Count           0.840            0.781          0.877\n",
       "Logit_TFIDF         0.858            0.850          0.922\n",
       "Logit_Count         0.821            0.861          0.900"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
