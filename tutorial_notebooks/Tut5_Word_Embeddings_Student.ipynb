{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5 - Word Embeddings\n",
    "\n",
    "In the lectures, so far you have covered NLP processing of textual data and word embeddings. Therefore, in today's notebook we will revisit the algorithm Word2Vec (W2V) that you can use to learn word embeddings with two models, i.e., CBOW and Skip-Gram. While the two flavours of W2V differ in how they are modeling token embeddings, both models aim at producing dense numerical vector representations, that capture the semantic relationships in the input textual samples. Once we have covered how to create pretrained embedding dictionaries compatible with keras, we will generate two-dimensional representation of the trained word embeddings that is suitable for plotting purposes. Afterward, we will use the embeddings from W2V in an MLP-based network trained for sentiment classification. <br>\n",
    "\n",
    "Here is the outline of today's notebook:\n",
    "*   Word2Vec: Implementation of an Embedding Layer Dictionary with CBOW and Skip-Gram (Demo). \n",
    "*   Plotting Embeddings using t-SNE (Exercise 1).\n",
    "*   MLP-based Neural Network with W2V Embeddings for Sentiment Classification (Exercise 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages:\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.metrics import recall_score,precision_score,roc_auc_score\n",
    "import numpy as np\n",
    "from keras.layers import TextVectorization\n",
    "from gensim.models import Word2Vec \n",
    "import time\n",
    "import keras\n",
    "from keras import Sequential\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Word2Vec: Implementation of an Embedding Layer Dictionary with CBOW and Skip-Gram** (Demo)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use W2V to train word embeddings, which capture the contextual meaning of individual tokens:<br>\n",
    "<img src=\"https://github.com/Humboldt-WI/demopy/raw/main/W2V.PNG\" width=\"1140\" height=\"610\" alt=\"W2V\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In W2V, each word is represented by two vectors of dimension *ùëë*, as words take both roles, i.e., context and target word. The word vectors are the parameters of the neural network, which we train on our corpus of textual data for language modeling purposes. The goal is to learn low-dimensional, dense representation of words as numerical continuous vectors, which enable ML models to understand the meaning and the semantics of words algorithmically. Therefore, language modeling can be regarded as the upstream task, whereas, e.g., sentiment classification using the pretrained embeddings from W2V would be the downstream task. There are two variants of W2V, which we can use to obtain word embeddings: <br>\n",
    "\n",
    "<img src=\"https://github.com/Humboldt-WI/demopy/raw/main/CBOW_and_SkipGram.PNG\" width=\"1450\" height=\"390\" alt=\"CBOW_and_SkipGram\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Continuous Bad of Word (CBOW) model differs from the Skip-Gram model mainly in that the former predicts the center word from surrounding context tokens, whereas the latter predicts the context from the center word. The surrounding words are quantified by defining a context window. In the above visualization, the context window is set to 2 tokens. The training process involves parsing the textual samples with context size 2, and sliding the training tuples consisting of inputs and targets until the end of the sentences. Language modeling does not necessitate target labels or text annotation, as the training inputs and outputs are obtained from parsing the textual samples. Thus, we call such training process self-supervised. W2V in its two flavours is trained with a shallow neural network. For simplicity purposes, assume you have a single token as the input and the following word as the target:<br>\n",
    "<img src=\"https://github.com/Humboldt-WI/demopy/raw/main/W2V_Shallow_Network_Example.PNG\" width=\"1450\" height=\"410\" alt=\"W2V_Shallow_Network_Example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer of the network represents a one-hot-vector, which has a positive value at the index of the input word. The dot product of the one-hot-vector with the continuous trainable weights of the shallow network amounts to indexing the vector of weights in the hidden layer associated with the input word *can*. This is also the vector of weights that we will use as our word embeddings in the downstream task. If we choose to train our own embeddings using W2V instead of downloading pretrained embeddings, then, first, we would have to clean our dataset. Thus, let's import the IMDB dataset, and clean it using our `NLP_preprocessing_pipeline` function from the previous tutorial notebook. While previously the function returned a list of cleaned tokens, in this notebook we will design our function to return the whole cleaned string of the textual samples, as we will later feed these cleaned strings to a `TextVectorization` layer from `keras`:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag (Parts of Speech tagging)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word:str)->str:\n",
    "    \"\"\"Map POS tag to first character for lemmatization\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pos: str\n",
    "        The positional tag of speech retrieved from wordnet database.\n",
    "    \"\"\"\n",
    "\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "                \n",
    "    pos=tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "    return pos\n",
    "\n",
    "def NLP_preprocessing_pipeline(textual_sample:str)->list:\n",
    "    '''\n",
    "    Implements 7 steps of an NLP preprocessing pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    textual_sample:str\n",
    "        The input text that requires preprocessing\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    preprocessed_textual_sample:str\n",
    "        The textual sample after each of the 7 preprocessing steps have been applied.\n",
    "\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    #Removing of URLs:\n",
    "    preprocessed_textual_sample = re.sub(\"http\\S+\", \"\",textual_sample)\n",
    "\n",
    "    #Removing of HMTL tags:\n",
    "    preprocessed_textual_sample = BeautifulSoup(preprocessed_textual_sample).get_text()\n",
    "\n",
    "    #Removing of non-alphabetic characters:\n",
    "    preprocessed_textual_sample = re.sub(\"[^a-zA-Z]\", \" \",preprocessed_textual_sample)\n",
    "\n",
    "    #Changing all tokens to lower case:\n",
    "    preprocessed_textual_sample = preprocessed_textual_sample.lower()\n",
    "\n",
    "    #Tokenization:\n",
    "    preprocessed_textual_sample=nltk.word_tokenize(preprocessed_textual_sample)\n",
    "\n",
    "    #Stopwords removal:\n",
    "    preprocessed_textual_sample = [w for w in preprocessed_textual_sample if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    #Lemmatization:\n",
    "    preprocessed_textual_sample=[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in preprocessed_textual_sample]\n",
    "    preprocessed_textual_sample=' '.join(preprocessed_textual_sample)\n",
    "    \n",
    "    return preprocessed_textual_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB-50K-Movie-Review.zip\", sep=\",\", encoding=\"ISO-8859-1\").iloc[:5000,:]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['review'].apply(NLP_preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will save the cleaned version of our movie reviews as the preprocessing takes a while to complete.\n",
    "with open('cleaned_X_str.pkl','wb') as f:\n",
    "    pickle.dump(X,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_X_str.pkl','rb') as f:\n",
    "    X=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will build our vocabulary with the `TextVectorization` layer. The latter transforms the input strings to a list of integer token indices, which are associated with a unique word in our vocabulary. We learn the vocabulary by calling the function `adapt`. When the layer is adapted, it learns the frequency of the individual tokens in the dataset. If we specify a maximum size of our vocabulary, e.g., 15k, then the layer would create a vocabulary containing the 15k most frequently encountered words in the cleaned textual samples. Words outside of this vocabulary get mapped to the UNK-token, i.e., out-of-vocabulary token. For consistency purposes, we will also specify the maximal sequence length for each textual sample. For instance, if we set the sequence length to a maximum of 100 tokens, then textual samples with less tokens will get padded with zeros to a length of 100. Similarly to other preprocessing techniques that we have applied so far in this semester, the layer `TextVectorization` is adapted on the train set only. For this reason, we will split our cleaned data *X* into train and test set before the learning of the vocabulary takes place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train and test subsets:\n",
    "y=df['sentiment'].map({'positive':1,'negative':0}).values  # map text-based class labels to numbers\n",
    "Xclean_train, Xclean_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5)  # data partitioning\n",
    "\n",
    "# Create a vectorization layer\n",
    "vocab_size = 15000\n",
    "seq_length = 100\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = None,\n",
    "    #since we have cleaned our data already, \n",
    "    # we pass None to the text standardization parameter.\n",
    "    max_tokens = vocab_size,\n",
    "    output_sequence_length = seq_length)\n",
    "\n",
    "#Learn the vocabulary on the train set:\n",
    "vectorize_layer.adapt(Xclean_train)\n",
    "\n",
    "#print the first 15 tokens in the vocabulary:\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(vocab[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the vectorization layer is a list of integers indicating the position of the words in the vocabulary with the most frequent tokens. Let's take the first clean sample in our test set, and transform it using the `TextVectorization` layer that we have already adapted on our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first clean textual sample in the test dataset:')\n",
    "print(Xclean_test.iloc[0],'\\n')\n",
    "print('The corresponding sentiment label: ',y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The output from the Text Vectorization: ')\n",
    "vectorize_layer(Xclean_test.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zeros correspond to an empty token resulting from the padding process, and the index 1 corresponds to the UNK token. Since we are interested in training our own embeddings with W2V, we will first vectorize our train textual samples. We will then use the output to retrieve the corresponding words in string format from our vocabulary, which would serve as the input to W2V. In this way, we would generate embeddings only for the 15k most frequent tokens in our vocabulary. If we feed the cleaned textual samples to W2V without the text vectorization step, then W2V might produce embeddings also for tokens that get mapped to the UNK token during the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec=vectorize_layer(Xclean_train)\n",
    "X_train_words = [[vocab[w] for w in rev if vocab[w] not in ['','[UNK]']] for rev in X_train_vec]\n",
    "#we collect all train words except '' and '[UNK]', as we do not have to learn embeddings for these two tokens.\n",
    "#When we create our keras embedding layer, we will overwrite all embeddings with those learned with W2V except the embeddings\n",
    "# for the first two tokens. The embeddings of the latter will be randomly initialized, as the tokens '' and '[UNK]' do not have \n",
    "# a specific contextual meaning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since this can take a while, we will save result so that we can import it later:\n",
    "with open('X_train_words.pkl','wb') as f:\n",
    "    pickle.dump(X_train_words,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train_words.pkl','rb') as f:\n",
    "    X_train_words=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will learn the word embeddings using both CBOW and Skip-Gram. Afterward, we will create embedding layers, the continuous vectors of which will be overwritten with the embeddings from W2V:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CBOW and SkipGram:\n",
    "#Keep track of the training time:\n",
    "embeddings_dimension=300\n",
    "start_time_cbow=time.time()\n",
    "w2v_model_cbow = Word2Vec(X_train_words, min_count=100, #we set the minimal token frequency to 100 to avoid creating embeddings for \n",
    "                        #rarely represented words\n",
    "                        window=10, #the size of context\n",
    "                        epochs=50,  \n",
    "                        vector_size=embeddings_dimension, #size of embedding\n",
    "                        workers=4,#for parallel computing\n",
    "                        sg  = 0)  \n",
    "end_time_cbow=time.time()\n",
    "\n",
    "start_time_skipgram=time.time()\n",
    "w2v_model_skipgram = Word2Vec(X_train_words, min_count=100, window=10,     \n",
    "                 epochs=50,  vector_size=embeddings_dimension, workers=4, sg  = 1)  \n",
    "\n",
    "end_time_skipgram=time.time()\n",
    "print('Training Time with CBOW in Seconds: ',round(end_time_cbow-start_time_cbow,5))\n",
    "print('Training Time with SkipGram in Seconds: ',round(end_time_skipgram-start_time_skipgram,5))  \n",
    "\n",
    "\n",
    "#Create corresponding Embedding Layers:\n",
    "embedding_layer_cbow=keras.layers.Embedding(vocab_size,embeddings_dimension)\n",
    "embedding_layer_cbow.build(input_shape=(embeddings_dimension))\n",
    "\n",
    "embedding_layer_skipgram= keras.layers.Embedding(vocab_size,embeddings_dimension)\n",
    "embedding_layer_skipgram.build(input_shape=(embeddings_dimension))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively to training your own embeddings, you can download pretrained embeddings usign the package gensim:\n",
    "# import gensim.downloader\n",
    "# w2v_model_cbow=gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overwrite the weights in the embedding layers with the continuous  token representation learned with CBOW and Skip-Gram:\n",
    " \n",
    "embeddings_cbow=[]\n",
    "embeddings_skipgram=[]\n",
    "\n",
    "#Fill in the embedding matrices:\n",
    "for token_idx in range(0,len(vocab)):\n",
    "    if token_idx>1:\n",
    "        if vocab[token_idx] in w2v_model_cbow.wv:\n",
    "            embeddings_cbow.append(w2v_model_cbow.wv[vocab[token_idx]])\n",
    "        else:#take embedding corresponding to UNK token:\n",
    "            embeddings_cbow.append(embedding_layer_cbow.get_weights()[0][1])\n",
    "\n",
    "        \n",
    "        if vocab[token_idx] in w2v_model_skipgram.wv:\n",
    "            embeddings_skipgram.append(w2v_model_skipgram.wv[vocab[token_idx]])\n",
    "        else:#take embedding corresponding to UNK token:\n",
    "            embeddings_skipgram.append(embedding_layer_skipgram.get_weights()[0][1])\n",
    "\n",
    "    else:#then take mebeddings of ''-token or '[UNK]'-token:\n",
    "        embeddings_cbow.append(embedding_layer_cbow.get_weights()[0][token_idx])\n",
    "        embeddings_skipgram.append(embedding_layer_skipgram.get_weights()[0][token_idx])\n",
    "\n",
    "#Overwrite the weights in the embedding layers with the corresponding W2V token embeddings.\n",
    "#You will need the embedding_layer_cbow and embedding_layer_skipgram for the 1. exercise.\n",
    "embedding_layer_cbow.set_weights(np.array([embeddings_cbow]))\n",
    "embedding_layer_skipgram.set_weights(np.array([embeddings_skipgram]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo Summary**:<br>\n",
    "- we can use W2V to train our own embeddings with two models, i.e., CBOW and Skip-Gram.<br>\n",
    "- the inputs to W2V are the cleaned tokenized textual samples coming from the vocabulary built with the text vectorization layer. The latter creates the vocabulary from the set of most frequently encountered words in our cleaned dataset.\n",
    "- once we have trained our embeddings, we save each continuous vector corresponding to a token in our vocabulary in embedding matrices that we use to overwrite the weights of the embedding layers in `keras`. These embedding layers serve as a dictionary to look up words based on their indices produced from the text vectorization transformation.\n",
    "- we can use these embedding layers when implementing neural networks in `keras` to process text algorithmically. Also, we can use the output of the embedding layers as inputs to dimensionality reduction techniques that produce low dimensional vectors for plotting purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Plotting Embeddings using t-SNE** (Exercise 1)<br>\n",
    "T-distributed Stochastic Neighbor Embedding (t-SNE) is a tool to generate low dimensional data, e.g., in a 2D space, from high dimensional embeddings for visualization purposes. The transformation process of t-SNE preserves the structure of the data in the low dimensional space, which enables the interpretability of the original high dimensional vectors. `Sklearn` provides you with easy access to the implementation of t-SNE (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). However, `sklearn`'s implementation is quite costly and can consume a lot of memory especially if the original data comes in a high dimensional space such as the embeddings from W2V. Therefore, in this task you will use a less computationally expensive version of t-SNE implemented in the package `openTSNE`. For this purpose, you would have to install the package with pip first. Once you have completed the installation of `openTSNE`, you are tasked with the following:<br>\n",
    "- take the first 500 samples from the cleaned `Xclean_train` dataset, and transform them with the `vectorize_layer` from the demo.\n",
    "- feed the vectorized output to the embedding layers containing the CBOW and Skip-Gram weights from the demo (`embedding_layer_cbow` and `embedding_layer_skipgram`), convert the Tensor to numpy format, and flatten the token sequences to obtain vectors of the dimensionality 300 (embedding dimension) * 100 (sequence length).\n",
    "- fit `TSNE` from `openTSNE` on the cbow and skip-gram embeddings, and store the resulting two components together with the target labels from the train set (`y_train`) in two dataframes. Name the columns of the dataframes X-Axis, Y-Axis and Sentiment Class.\n",
    "- use the package `seaborn` to plot two well-annotated scatter plots next to each other for each embedding type. Use the parameter hue from `seaborn`'s scatter plot to plot the relationship of the two-dimensional t-SNE embeddings to the sentiment class. You can check out some examples for plotting with seaborn under the following link: https://seaborn.pydata.org/generated/seaborn.scatterplot.html.  \n",
    "- provide a short interpretation of the visualization: what pattern do you observe? what could be causing this pattern? in which scenario would you observe a different pattern?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openTSNE\n",
    "from openTSNE import TSNE\n",
    "num_samples=500\n",
    "#Get the embeddings using the vectorize layer and the embeddings layers from the demo:\n",
    "cbow_reviews_train=...\n",
    "#flatten the last two dimensions:\n",
    "cbow_reviews_train=...\n",
    "cbow_tsne_embeddings = TSNE().fit(...)\n",
    "\n",
    "cbow_tsne_embeddings=pd.DataFrame({...})\n",
    "\n",
    "skipgram_reviews_train=...\n",
    "#flatten the last two dimensions:\n",
    "skipgram_reviews_train=...\n",
    "skipgram_tsne_embeddings = TSNE().fit(...)\n",
    "skipgram_tsne_embeddings=pd.DataFrame({...})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(20,5))\n",
    "sns.scatterplot(x=...,y=...,data=cbow_tsne_embeddings,hue=...,ax=ax[0])\n",
    "ax[0].set_xlabel(...)\n",
    "ax[0].set_ylabel(...)\n",
    "ax[0].legend(mode='expand',ncols=2,loc=[0.0,-0.3],title='Sentiment Class')\n",
    "ax[0].set_title(...)\n",
    "\n",
    "sns.scatterplot(x=...,y=...,data=skipgram_tsne_embeddings,hue=...,ax=ax[1])\n",
    "ax[1].set_xlabel(...)\n",
    "ax[1].set_ylabel(...)\n",
    "ax[1].set_title(...)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:<br>\n",
    "... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. MLP-based Neural Network with W2V Embeddings for Sentiment Classification** (Exercise 2)<br>\n",
    "In this exercise, you are tasked with the implementation of two MLP-based neural networks for sentiment classification on the IMDB dataset using the W2V embeddings that we generated with CBOW and Skip-Gram in the demo part of this notebook:<br>\n",
    "- build two sequential keras models (cbow_mlp_model and skipgram_mlp_model), which consists of six layers: <br>\n",
    "an input layer with shape = (1) and  dtype='string', the `vectorize_layer` from the demo, an embedding layer initialized with the vocabulary size = 15k, the embedding dimension = 300 and the input sequence length = 100, a GlobalAveragePooling1D layer, a GELU-based hidden layer with 10 units and a final Sigmoid-based prediction layer.<br>\n",
    "\n",
    "- after compiling the models with the Adam optimizer and the binary crossentropy loss, overwrite the weights in the embedding layers with the embedding matrices from the demo `embeddings_cbow` and `embeddings_skipgram` by using the function `set_weights()`.\n",
    "- make two runs with each model for 10 epochs with a batch size of 32: in the first run freeze the W2V weights, in the second run set the weights in the embedding layers to trainable. You should use the cleaned version of the train and test datasets from the demo when fitting the models, i.e, `Xclean_train` and `Xclean_test`.\n",
    "\n",
    "- report the results in terms of AUC score, precision and recall score in a tabular overview. You can use the threshold of 0.5, so that you convert the keras probabilities into binary classes for the computation of the precision and recall scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_results_frame=[]\n",
    "\n",
    "for nr_run in [0,1]:\n",
    "  #Create the models: based on the run (0 or 1) set the trainable parameter of the Embedding layer to True or False\n",
    "  cbow_mlp_model = Sequential([...])\n",
    "\n",
    "  skipgram_mlp_model = Sequential([...])\n",
    "\n",
    "  #Compile the models and overwrite the embedding layer weights with the embeddings from W2V:\n",
    "  cbow_mlp_model.compile(...)\n",
    "  cbow_mlp_model.layers[1].set_weights(np.array([embeddings_cbow]))\n",
    "  \n",
    "  skipgram_mlp_model.compile(...)\n",
    "  skipgram_mlp_model.layers[1].set_weights(np.array([embeddings_skipgram]))\n",
    "\n",
    "  #Fit the cbow-based model and make predictions:\n",
    "  cbow_mlp_model.fit(Xclean_train.to_numpy().reshape(-1,1), \n",
    "                     y_train,\n",
    "                     ...)\n",
    "\n",
    "  cbow_proba_pr=cbow_mlp_model.predict(Xclean_test.to_numpy().reshape(-1,1),verbose=0).flatten()\n",
    "  cbow_class_pr=np.where(...)\n",
    "  predictions_results_frame.append([...])\n",
    "\n",
    "  #Do the same for the skipgram-based MLP:\n",
    "  skipgram_mlp_model.fit(Xclean_train.to_numpy().reshape(-1,1), \n",
    "                         y_train,\n",
    "                         ...)\n",
    "\n",
    "  skipgram_proba_pr=skipgram_mlp_model.predict(Xclean_test.to_numpy().reshape(-1,1),verbose=0).flatten()\n",
    "  skipgram_class_pr=np.where(...)\n",
    "  predictions_results_frame.append([...])\n",
    "\n",
    "#Put the results in a dataframe:\n",
    "results_overview=pd.DataFrame(np.around(np.array(predictions_results_frame),3),columns=['Recall Score','Precision Score','ROC Auc Score'],\n",
    "            index=[...])#name the indices in such way that it is clear which run was using trainable vs. non-trainable embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_overview.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
